{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa8cd98",
   "metadata": {},
   "source": [
    "\n",
    "### Ethereum Address Scan is an intuitive tool designed for users to easily retrieve detailed information about any Ethereum address. Just input an Ethereum address, and get a comprehensive overview in a user-friendly format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606f61f",
   "metadata": {},
   "source": [
    "## Full Project Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126d991",
   "metadata": {},
   "source": [
    "### Install the necesarry libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb7c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (1.44.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (2.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: certifi in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2024.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc699e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: openai[datalib]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai[datalib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1615b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.6\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl.metadata (44 kB)\n",
      "Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "types-requests 2.32.0.20240914 requires urllib3>=2, but you have urllib3 1.26.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-1.26.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3==1.26.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646f69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26816d3-cef3-4c39-a901-cd7a49d3fcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shmulik/Documents/jupyter/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48e61b",
   "metadata": {},
   "source": [
    "### Import the libraries and enviornment file to gain access to the Open API Key\n",
    "#### The key can be generated here: https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70841de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5387008",
   "metadata": {},
   "source": [
    "### Authenticate to the API using the API Key\n",
    "#### Pull from environment variables or use openai.api_key = (\"your_key_here\") to hardcode the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf566e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=\"your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9e854-9d65-46b9-b08d-7112da95889f",
   "metadata": {},
   "source": [
    "### Helper Functions, taken from https://platform.openai.com/docs/guides/fine-tuning docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5568135-24bf-4c5d-83ee-47e20d15113f",
   "metadata": {},
   "source": [
    "### Convert JSON to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7225dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted eth_addresses.csv to output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Read CSV and Convert to JSONL\n",
    "csv_file = 'eth_addresses.csv'  # Replace with your CSV file path\n",
    "jsonl_file = 'output.jsonl'  # Output JSONL file path\n",
    "\n",
    "# Open CSV file and JSONL output file\n",
    "with open(csv_file, mode='r', newline='', encoding='utf-8') as csvfile, open(jsonl_file, mode='w', encoding='utf-8') as jsonlfile:\n",
    "    # Read CSV using DictReader to maintain column names\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Convert each row to JSON and write to the JSONL file\n",
    "        json.dump(row, jsonlfile)\n",
    "        jsonlfile.write('\\n')\n",
    "\n",
    "print(f\"Successfully converted {csv_file} to {jsonl_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78fa9d7-908a-42eb-81eb-69d10d65b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted eth_addresses.csv to output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Input CSV file and output JSONL file paths\n",
    "csv_file = 'eth_addresses.csv'  # Replace with your CSV file path\n",
    "jsonl_file = 'output.jsonl'  # Desired output JSONL file path\n",
    "\n",
    "# Open CSV and JSONL files\n",
    "with open(csv_file, mode='r', newline='', encoding='utf-8') as csvfile, open(jsonl_file, mode='w', encoding='utf-8') as jsonlfile:\n",
    "    # Read the CSV file using DictReader\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Construct the JSON structure based on the required format\n",
    "        message = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"This tool verifies Ethereum addresses. Simply enter the address you want to retrieve details about\"},\n",
    "                {\"role\": \"user\", \"content\": row['Address']},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f'\"Name\": \"{row[\"Name\"]}\", '\n",
    "                               f'\"Account Type\": \"{row[\"Account Type\"]}\", '\n",
    "                               f'\"Contract Type\": \"{row[\"Contract Type\"]}\", '\n",
    "                               f'\"Entity\": \"{row[\"Entity\"]}\", '\n",
    "                               f'\"Label\": \"{row[\"Label\"]}\", '\n",
    "                               f'\"Tags\": \"{row[\"Tags\"]}\"'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write each message object as a new line in JSONL format\n",
    "        jsonlfile.write(json.dumps(message) + \"\\n\")\n",
    "\n",
    "print(f\"Successfully converted {csv_file} to {jsonl_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754c20-deb7-4e3f-8e57-e9907dd6f132",
   "metadata": {},
   "source": [
    "### Check File Format\n",
    "\n",
    "https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16904d3c-0b0f-457b-a3ad-0fcb908f5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 19138\n",
      "First example:\n",
      "{'role': 'user', 'content': '0x8ab7404063ec4dbcfd4598215992dc3f8ec853d7'}\n",
      "{'role': 'assistant', 'content': '\"Name\": \" Akropolis (AKRO)\", \"Account Type\": \"Smart Contract\", \"Contract Type\": \"Token\", \"Entity\": \"DeFi\", \"Label\": \"Legit\", \"Tags\": \"DeFi\"'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"output.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3a89f4e-b528-4b4e-b687-8458777224af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format validation\n",
    "check_file_format(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9836ae0-1c33-4ba0-8a88-03fda5ea0c5a",
   "metadata": {},
   "source": [
    "### Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a3dc2da-9a08-4fbc-96a7-28ab292e513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~1473815 tokens that will be charged for during training\n",
      "By default, you'll train for 1 epochs on this dataset\n",
      "By default, you'll be charged for ~1473815 tokens\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the conversation\n",
    "conversation_length = []\n",
    "\n",
    "for msg in dataset:\n",
    "    messages = msg[\"messages\"]\n",
    "    conversation_length.append(num_tokens_from_messages(messages))\n",
    "    \n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "TARGET_EPOCHS = 5\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in conversation_length)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "num_tokens = n_epochs * n_billing_tokens_in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0c234a-e9e0-40ee-a7f7-5e03bb768b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.421445\n"
     ]
    }
   ],
   "source": [
    "# gpt-4o-mini-2024-07-18 cost is $0.0030 / 1K tokens\n",
    "cost = (num_tokens/1000) * 0.0030 \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd157b0f-dfaa-42a9-a4e5-11ba884efaf3",
   "metadata": {},
   "source": [
    "### Upload File \n",
    "#### Once you have the data validated, the file needs to be uploaded using the \n",
    "#### Files API in order to be used with a fine-tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c62c42-90a2-480e-b55d-8c58f26dfa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-44IgH9zSsw2N1UlRmAfpg0nF', bytes=5519454, created_at=1728295894, filename='output.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(\n",
    "  file=open(\"output.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd054dc",
   "metadata": {},
   "source": [
    "### Create fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21da05e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-jjY1hv3Z3mc5sbp9J0xLW1I2', created_at=1728295982, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=2, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-Xpq7QD5G5VDB9ibrYpVDO9at', result_files=[], seed=968622431, status='validating_files', trained_tokens=None, training_file='file-44IgH9zSsw2N1UlRmAfpg0nF', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the fine-tuning job \n",
    "# After you've started a fine-tuning job, it may take some time to complete. Your job may be queued \n",
    "# behind other jobs and training a model can take minutes or hours depending on the \n",
    "# model and dataset size. \n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-44IgH9zSsw2N1UlRmAfpg0nF\", \n",
    "  model=\"gpt-4o-mini-2024-07-18\",\n",
    "  hyperparameters={\n",
    "    \"n_epochs\":3\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ebec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-jjY1hv3Z3mc5sbp9J0xLW1I2', created_at=1728295982, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=2, batch_size=25, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-Xpq7QD5G5VDB9ibrYpVDO9at', result_files=[], seed=968622431, status='validating_files', trained_tokens=None, training_file='file-44IgH9zSsw2N1UlRmAfpg0nF', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve job status\n",
    "job_id = \"ftjob-jjY1hv3Z3mc5sbp9J0xLW1I2\"\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "# Status field can contain: running or succeeded or failed, etc.\n",
    "client.fine_tuning.jobs.retrieve(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd98e42-8dac-438b-a622-4f361d0434f1",
   "metadata": {},
   "source": [
    "### Evaluate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd35ca4d-3a41-42d8-baad-1ee0e219158a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.62449</td>\n",
       "      <td>0.74194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.58815</td>\n",
       "      <td>0.62963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.68213</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.31334</td>\n",
       "      <td>0.60870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.89790</td>\n",
       "      <td>0.61290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>501</td>\n",
       "      <td>0.16619</td>\n",
       "      <td>0.92593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>502</td>\n",
       "      <td>0.49473</td>\n",
       "      <td>0.79412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>503</td>\n",
       "      <td>0.24529</td>\n",
       "      <td>0.90909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>504</td>\n",
       "      <td>0.28140</td>\n",
       "      <td>0.89286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>505</td>\n",
       "      <td>0.45056</td>\n",
       "      <td>0.84848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy\n",
       "0       1     1.62449         0.74194         NaN                        NaN\n",
       "1       2     1.58815         0.62963         NaN                        NaN\n",
       "2       3     1.68213         0.60000         NaN                        NaN\n",
       "3       4     2.31334         0.60870         NaN                        NaN\n",
       "4       5     1.89790         0.61290         NaN                        NaN\n",
       "..    ...         ...             ...         ...                        ...\n",
       "500   501     0.16619         0.92593         NaN                        NaN\n",
       "501   502     0.49473         0.79412         NaN                        NaN\n",
       "502   503     0.24529         0.90909         NaN                        NaN\n",
       "503   504     0.28140         0.89286         NaN                        NaN\n",
       "504   505     0.45056         0.84848         NaN                        NaN\n",
       "\n",
       "[505 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "#once training is finished, you can retrieve the file in \"result_files=[]\"\n",
    "result_file = \"file-44IgH9zSsw2N1UlRmAfpg0nF\"\n",
    "\n",
    "file_data = client.files.content(result_file)\n",
    "\n",
    "# its binary, so read it and then make it a file like object\n",
    "file_data_bytes = file_data.read()\n",
    "file_like_object = io.BytesIO(file_data_bytes)\n",
    "\n",
    "#now read as csv to create df\n",
    "df = pd.read_csv(file_like_object)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487724c-6f37-438a-86c9-35b39d18992e",
   "metadata": {},
   "source": [
    "### Iterate on the Model results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa36376-915c-4871-aa6e-568ee9498755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-ddj5DgpOc0khmh3OCDkUxytC', created_at=1709004655, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=4, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-RZLvEijW4GW0KmC3rLIAjZlu', result_files=[], status='validating_files', trained_tokens=None, training_file='file-IntFuYDWVfJwMp6TpSrJa8aq', validation_file=None, user_provided_suffix=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-44IgH9zSsw2N1UlRmAfpg0nF\", \n",
    "  model=\"gpt-4o-mini-2024-07-18\",\n",
    "  hyperparameters={\n",
    "    \"n_epochs\":3\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78db69bc-acad-47c9-a9b7-5d8b56b60590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-ddj5DgpOc0khmh3OCDkUxytC', created_at=1709004655, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0613:keysoft::8wiiPbKa', finished_at=1709005600, hyperparameters=Hyperparameters(n_epochs=4, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-RZLvEijW4GW0KmC3rLIAjZlu', result_files=['file-rhw44JG1hrIRtpqXjGE0PK7C'], status='succeeded', trained_tokens=27388, training_file='file-IntFuYDWVfJwMp6TpSrJa8aq', validation_file=None, user_provided_suffix=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve job status\n",
    "job_id = \"ftjob-ddj5DgpOc0khmh3OCDkUxytC\"\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "# Status field can contain: running or succeeded or failed, etc.\n",
    "client.fine_tuning.jobs.retrieve(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65867f2a-bb96-4dd5-8c83-464e937b6119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.65891</td>\n",
       "      <td>0.72727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.77342</td>\n",
       "      <td>0.77778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.66960</td>\n",
       "      <td>0.76923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.85210</td>\n",
       "      <td>0.81081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.34291</td>\n",
       "      <td>0.60870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>1.03416</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>401</td>\n",
       "      <td>0.75915</td>\n",
       "      <td>0.75862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>402</td>\n",
       "      <td>0.38260</td>\n",
       "      <td>0.83871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>403</td>\n",
       "      <td>0.43998</td>\n",
       "      <td>0.86207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>404</td>\n",
       "      <td>0.36769</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy\n",
       "0       1     0.65891         0.72727         NaN                        NaN\n",
       "1       2     0.77342         0.77778         NaN                        NaN\n",
       "2       3     1.66960         0.76923         NaN                        NaN\n",
       "3       4     0.85210         0.81081         NaN                        NaN\n",
       "4       5     2.34291         0.60870         NaN                        NaN\n",
       "..    ...         ...             ...         ...                        ...\n",
       "399   400     1.03416         0.66667         NaN                        NaN\n",
       "400   401     0.75915         0.75862         NaN                        NaN\n",
       "401   402     0.38260         0.83871         NaN                        NaN\n",
       "402   403     0.43998         0.86207         NaN                        NaN\n",
       "403   404     0.36769         0.90000         NaN                        NaN\n",
       "\n",
       "[404 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#once training is finished, you can retrieve the file in \"result_files=[]\"\n",
    "result_file = \"file-44IgH9zSsw2N1UlRmAfpg0nF\"\n",
    "\n",
    "file_data = client.files.content(result_file)\n",
    "\n",
    "# its binary, so read it and then make it a file like object\n",
    "file_data_bytes = file_data.read()\n",
    "file_like_object = io.BytesIO(file_data_bytes)\n",
    "\n",
    "#now read as csv to create df\n",
    "df = pd.read_csv(file_like_object)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a287e95",
   "metadata": {},
   "source": [
    "### Use a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828defbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, sharing a cryptocurrency address like 0x9d5765ae1c95c21d4cc3b1d5bba71bad3b012b68 is not risky as it is a public identifier for a wallet. However, make sure not to share your private key with anyone.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:personal::AFatPoOi\"  \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=fine_tuned_model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Ethereum Address Scan is an intuitive tool designed for users to easily retrieve detailed information about any Ethereum address. Just input an Ethereum address, and get a comprehensive overview in a user-friendly format.\",\n",
    "     \"role\": \"user\", \"content\": \"Is this address risky - 0x9d5765ae1c95c21d4cc3b1d5bba71bad3b012b68 ?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
